{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first NLP lesson, we introduced unsupervised topic extraction through Latent Semantic Analysis (LSA), in which principal components analysis is run on a tf-idf matrix.  The resulting components are considered 'topics' within the text.  In this assignment, we are going to review some additional topic extraction methods.  All of these methods requite the user to specify in advance the number of topics to extract.\n",
    "\n",
    "# What is in a topic?  The problem of interpretability.\n",
    "\n",
    "A problem with LSA is that the link between words and topics isn't very clear.  Some words will have high positive loadings on a component, while other words will have high negative loadings on a component.  It's easy to understand that a high positive loading means that those words are highly relevant to the topic represented by that component.  But what does a high negative loading mean?  Is the topic linked to the opposite of the negative words in some way?  This lack of clarity can make it hard to understand exactly what topic a component represents.\n",
    "\n",
    "Here is an example, drawn from the LSA lesson.  We take Emma by Jane Austen, parse it, convert it to a tf-idf matrix, then run a PCA on it (using the TruncatedSVD command so that it won't center the data and remove sparsity).  Then we'll  look at the relationship between a few words and the set of topics/components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD3CAYAAADSftWOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x156cc576b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "#reading in the data, this time in the form of paragraphs\n",
    "emma=gutenberg.paras('austen-emma.txt')\n",
    "#processing\n",
    "emma_paras=[]\n",
    "for paragraph in emma:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    emma_paras.append(' '.join(para))\n",
    "\n",
    "# Creating the tf-idf matrix.\n",
    "vectorizer = TfidfVectorizer()\n",
    "emma_paras_tfidf=vectorizer.fit_transform(emma_paras)\n",
    "\n",
    "# Generating 130 topics.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "emma_paras_lsa = lsa.fit_transform(emma_paras_tfidf)\n",
    "\n",
    "# Getting the word list.\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# Loading scores for each word on each topic/component.\n",
    "words_by_topic=emma_paras_tfidf.T * emma_paras_lsa\n",
    "\n",
    "# Linking the loadings to the words in an easy-to-read way.\n",
    "components=pd.DataFrame(words_by_topic,index=terms)\n",
    "\n",
    "components.loc['marriage'].hist()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA and words\n",
    "\n",
    "As shown above, the word 'marriage' features in several topics (the parts of the histogram where the loading is not zero).  What does that mean?  Are topics with negative loadings for 'marriage' topics having to do with the opposite of  marriage (such as divorce), or are they just totally unrelated to marriage (such as turnips)?   There's no way to know without examining every topic individually.  \n",
    "\n",
    "The point of this is to show that LSA is useful as a data reduction tool, but if you want to be able to understand how the topics relate to the words in your corpus, it is not the best technique because of its use of negative loadings.  Unfortunately, negative loadings are an inherent part of PCA.  Fortunately, there are other methods better suited to dealing with sparse, non-negative matrices.\n",
    "\n",
    "# Topic modeling with LSA variants\n",
    "\n",
    "## Probabilistic LSA\n",
    "\n",
    "In LSA, the goal is to reproduce the tf-idf matrix using a smaller set of components, which we then call 'topics.'  A modification, *probabilistic* LSA (pLSA, also called probabilistic Latent Semantic Indexing, or pLSI) approaches the problem in the opposite direction.  pLSA begins by assuming the existence of a set of topics, that set being unknown at the start (hidden variables that are believed to influence the data are called *latent* variables).  A document that contains a topic will have a high probability of containing some words (related to the topic), and a low probability of containing other words (unrelated to the topic).  Documents also have a probability of featuring each topic, low for some topics and high for others.\n",
    "\n",
    "In equation form, we are saying that the probability of a word $w$ appearing in a given document $d$ is a function of the  probability of a word occurring given a topic $t$ times the probabiltiy of a topic occurring given that document, summed across topics, times the probability of the given document.\n",
    "\n",
    "$$P(d,w)=P(d)\\sum_t P(w|t)P(t|d)$$\n",
    "\n",
    "Now, the nice thing is that we can easily calculate $P(d,w)$ and $P(d)$.  The probability of a word occurring in a document is the number of times the word appears in the document divided by the total number of words in the document.  the probability of a document, in turn, is the inverse of the number of documents in our corpus.  \n",
    "\n",
    "Having those things, we can then look for topics/word/document combinations that maximize the likelihood of getting the arrangement of documents and words that we observe in our data.  This idea of *maximizing likelihood* is also called **expectation maximization**.  It is an iterative process where each iteration has two steps.  In the first step (called the *E* step), the algorithm proposes a probability distribution for the topics based on the current model.  In the second step (the *M* step), the model parameters are re-estimated on the basis of the new probability distribution.    \n",
    "\n",
    "(Confusing?  Basically, we start by proposing a set of topics, then look at how likely the data we have would be given those topics.  Then we adjust our model based on what we learn, leading to new topics.  This is the opposite of LSA, where we start with the data and solve for a set of component-topics.)\n",
    "\n",
    "Iteration continues until the model reaches stopping conditions as defined by hyperparameters.  The most common stopping condition is when the change in the logged likelihood of the data between two iterations falls below a certain threshold.\n",
    "\n",
    "Like other algorithms based on iteration, such as gradient descent, *EM* solutions may not always converge on gloablly optimal parameter estimates.  Varying the starting parameter estimates can help to avoid this problem- if the model converges to similar or identical parameter estimates given different sets of starting parameters, the solution is more likely to be globally optimal.\n",
    "\n",
    "pLSA solves for a unique topic distribution for each document ($P(t|d)$).  This leads to some drawbacks:\n",
    "\n",
    "* the number of parameters increases linearly with the number of documents\n",
    "* high risk of overfitting \n",
    "* model does not generalize to new documents, only those in the corpus at the time the  model is fitted\n",
    "\n",
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Latent Dirichlet Allocation is a Bayesian implementation of pLSA that includes sparse Dirichlet priors for estimating the probability that a topic will be in a document ($P(t|d)$) and the probability that a word will be in a topic ($P(w|t)$).  Let's break that down:\n",
    "\n",
    "* The sparsity element is important because it tells the model that each document should be composed of only a few topics (with all other topic probabilities set to 0), and each topic should be composed of only a few words that are used frequently.  \n",
    "\n",
    "* The 'Dirichlet' indicates that the distribution is continuous, multivariate, and bounded at 0 and 1 (so it represents probabilities).  \n",
    "\n",
    "* The 'prior' gives information about the suspected likelihood of potential values for the parameters.  In pLSA, all potential values for $P(t|d)$ and $P(w|t)$ are considered equally likely because there is no prior.  \n",
    "\n",
    "LDA is computationally intensive to model, but can be more accurate than pLSA, with less proneness to overfitting.  The model also generalizes to new documents, and so is useful in a situation where the corpus is expected to expand with time.\n",
    "\n",
    "\n",
    "# Topic modeling with Non-Negative Matrix Factorization\n",
    "\n",
    "Latent variables are all very nice, but the need to model such a large number of unknowns makes methods like pLSA and LDA computationally unwieldy.  What if we could have the elegant simplicity of PCA but without the hassle of uninterpretable negative loadings?  That's where **Non-Negative Matrix Factorization** (NNMF) comes in.\n",
    "\n",
    "Like PCA, NNMF is searching for two matrices that, when multiplied together, result in the original data matrix- in this case, the tf-idf matrix.  \n",
    "\n",
    "Unlike PCA, however, in NNMF we apply the constraint that all three matrices (the original and the two new ones) must contain no negative values.  This constraint is why we say that $WH$ will be approximately equal ($\\approx$) rather than exactly equal ($=$):\n",
    "\n",
    "$$tfidf \\approx WH$$\n",
    "\n",
    "In text modeling, $W$ represents the topics, and $H$ describes whether, and to what extent, each topic applies to each document.\n",
    "\n",
    "There are a number of different way to fit an NNMF model.  Generally, the aim is to minimize the cost function $tfidf-WH$, that is, to find values for $W$ and $H$ that result in a matrix as close to $tfidf$ as possible.  Minimization occurs by iteration.  Many different iteration approaches can be used- ´sklearn´ implements NNMF with a coordinate descent algorithm.  \n",
    "\n",
    "In coordinate descent, different elements of the equation are optimized in turn.  Since NNMF is trying to optimize two matrices ($W$ and $H$), a coordinate descent implementation alternates updating $W$ and $H$.  In each update, new values are chosen that shrink the cost function relative to the previous values.  This approach is reminiscent of gradient descent, which we encountered in an earlier lesson.\n",
    "\n",
    "Like other iterative models, NNMF carries the risk of converging on a local, rather than global, minimum.  Different combinations of starting values can be tried to test whether the solution is likely to be global.  A strength of NNMF is that the solution is not only interpretable, but sparse-- most documents are linked to only a few topics, and most topics are linked to only a few words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a method\n",
    "\n",
    "There is no hard-and-fast rule for choosing a topic extraction method.  Under many circumstances, pLSA and LDA will yield highly similar solutions- since pLSA is faster than LDA, it may be preferred.  If overfitting is a concern, LDA or NNMF may be better choices than pLSA.  NNMF is generally faster than the other solutions.  \n",
    "\n",
    "Given that pLSA, LDA, and NNMF are all vulnerable to converging on a solution that is locally, but not globally, optimal, it may be advisable to apply multiple methods to see whether they yield similar solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
