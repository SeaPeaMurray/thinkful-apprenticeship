{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "### Build your own NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg, stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Emma doi*g just what she liked;\\\\*highly esteemi*g Miss Taylor's judgme*t, but directed chiefly by\\\\*her ow*.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick test after noticing odd apostrophe escapement behavior\n",
    "print(r\"'\" == r'\\'')\n",
    "test = r\"Emma doing just what she liked;\\nhighly esteeming Miss Taylor's judgment, but directed chiefly by\\nher own.\"\n",
    "re.sub(\"n\", '*', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_sentence</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7400</th>\n",
       "      <td>his face was an exceedingly round but sober on...</td>\n",
       "      <td>moby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7470</th>\n",
       "      <td>with back to the stranger ship and face set li...</td>\n",
       "      <td>moby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11778</th>\n",
       "      <td>and she had difficulty in behaving with temper</td>\n",
       "      <td>emma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8974</th>\n",
       "      <td>now as it shortly turned out what made this in...</td>\n",
       "      <td>moby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9221</th>\n",
       "      <td>but it is a mild mild wind and a mild looking ...</td>\n",
       "      <td>moby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10935</th>\n",
       "      <td>said she that is the only security for its fre...</td>\n",
       "      <td>emma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>folio and now begins book ii</td>\n",
       "      <td>moby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>chapter   the street</td>\n",
       "      <td>moby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15358</th>\n",
       "      <td>there is no admiration between them i do assur...</td>\n",
       "      <td>emma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13626</th>\n",
       "      <td>no no said she you are quite unreasonable</td>\n",
       "      <td>emma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_sentence source\n",
       "7400   his face was an exceedingly round but sober on...   moby\n",
       "7470   with back to the stranger ship and face set li...   moby\n",
       "11778     and she had difficulty in behaving with temper   emma\n",
       "8974   now as it shortly turned out what made this in...   moby\n",
       "9221   but it is a mild mild wind and a mild looking ...   moby\n",
       "10935  said she that is the only security for its fre...   emma\n",
       "2640                        folio and now begins book ii   moby\n",
       "797                                 chapter   the street   moby\n",
       "15358  there is no admiration between them i do assur...   emma\n",
       "13626          no no said she you are quite unreasonable   emma"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moby = gutenberg.raw('melville-moby_dick.txt')\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "def cleanup(frame, source):\n",
    "    # Removes the '\\r\\n' items across the text\n",
    "    frame = re.sub('\\r\\n', ' ', frame)\n",
    "    # The following two remove '--' which can have undesired effects\n",
    "    frame = re.sub('--', '-', frame)\n",
    "    frame = re.sub('--', '-', frame)\n",
    "    # Removes '\\n' across the text\n",
    "    frame = re.sub(r\"\\\\\", '', frame)\n",
    "    frame = re.sub('\\n', ' ', frame)\n",
    "    # The following two remove volume and chapter titles\n",
    "    frame = re.sub(r'CHAPTER [A-Z]', '', frame)\n",
    "    frame = re.sub(r'VOLUME [A-Z]', '', frame)\n",
    "    # Tokenize by sentence with nltk\n",
    "    frame_sents = sent_tokenize(frame)\n",
    "    # Create empty list to populate with clean tokens\n",
    "    frame_sents2 = []\n",
    "    for sent in frame_sents:\n",
    "        frame_sents2.append((re.sub('[^a-zA-Z\\' -]+', '', sent).lower()))\n",
    "    df = pd.DataFrame(frame_sents2, columns=['original_sentence'])\n",
    "    df['source'] = source\n",
    "    return df\n",
    "\n",
    "moby_df = cleanup(moby, 'moby')\n",
    "emma_df = cleanup(emma, 'emma')\n",
    "\n",
    "def clean_title(moby_, emma_):\n",
    "    moby_.loc[0]['original_sentence'] = 'etymology'\n",
    "    emma_.loc[0]['original_sentence'] = 'Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her.'\n",
    "    return pd.concat([moby_, emma_], ignore_index=True)\n",
    "\n",
    "df = clean_title(moby_df, emma_df)\n",
    "# tfidf copy\n",
    "df2 = df.copy()\n",
    "# copy to improve\n",
    "df3 = df.copy()\n",
    "# Let's take a look across the dataframe\n",
    "df.loc[np.random.randint(len(df), size=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using bag of words approach on the entire corpus assumes there are no words unique to one collection\n",
    "# Here we set vectorizers and fit them\n",
    "vec = CountVectorizer(ngram_range=(1,2), stop_words='english', max_features=100)\n",
    "tvec = TfidfVectorizer(ngram_range=(1,2), stop_words='english', max_features=100)\n",
    "bow = vec.fit_transform(df['original_sentence'])\n",
    "tfidf = tvec.fit_transform(df['original_sentence'])\n",
    "\n",
    "# This series of steps \n",
    "for name in vec.get_feature_names():\n",
    "    df[name] = 0\n",
    "    \n",
    "for i, j in zip(*bow.nonzero()):\n",
    "    df.loc[df.index[i], vec.get_feature_names()[j]] = bow[i, j]\n",
    "\n",
    "X = df[vec.get_feature_names()]\n",
    "y = df['source']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)#, stratify=y)\n",
    "\n",
    "# These similar steps replicate the process for tfidf\n",
    "for name in tvec.get_feature_names():\n",
    "    df2[name] = 0\n",
    "\n",
    "for i, j in zip(*tfidf.nonzero()):\n",
    "    df2.loc[df2.index[i], tvec.get_feature_names()[j]] = tfidf[i, j]\n",
    "    \n",
    "X2 = df2[tvec.get_feature_names()]\n",
    "y2 = df2['source']\n",
    "    \n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3)#, stratify=y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seapea\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7940609332819129"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_log = LogisticRegression()\n",
    "clf_log.fit(X_train, y_train)\n",
    "clf_log.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7759352101812572"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tree = DecisionTreeClassifier()\n",
    "clf_tree.fit(X_train, y_train)\n",
    "clf_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seapea\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7909757038179714"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_log2 = LogisticRegression()\n",
    "clf_log2.fit(X_train2, y_train2)\n",
    "clf_log2.score(X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738141149247976"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tree2 = DecisionTreeClassifier()\n",
    "clf_tree2.fit(X_train2, y_train2)\n",
    "clf_tree2.score(X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec2 = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                        min_df=4, # only use words that appear at least three times\n",
    "                        stop_words='english', \n",
    "                        lowercase=True, # this shouldn't be necessary, but still\n",
    "                        use_idf=True, # we definitely want to use inverse document frequencies in our weighting\n",
    "                        norm=u'l2', # Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                        smooth_idf=True, # Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                        ngram_range=(1,3) # This creates n-grams of 1 and 2\n",
    "                       )\n",
    "\n",
    "final = tvec2.fit_transform(df3['original_sentence'].to_list())\n",
    "\n",
    "# for name in tvec2.get_feature_names():\n",
    "#     df3[name] = 0\n",
    "\n",
    "# for i, j in zip(*final.nonzero()):\n",
    "#     df3.loc[df3.index[i], tvec2.get_feature_names()[j]] = tfidf[i, j]\n",
    "    \n",
    "X3 = final #df3[tvec.get_feature_names()]\n",
    "y3 = df3['source']\n",
    "    \n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(100)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "train_lsa = lsa.fit_transform(X_train3)\n",
    "test_lsa = lsa.fit_transform(X_test3)\n",
    "# y_lsa = lsa.fit_transform\n",
    "# lsa.score(X_test3, y_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seapea\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_log_final = LogisticRegression()\n",
    "clf_log_final.fit(train_lsa, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6650597763208639"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_log_final.score(test_lsa, y_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
