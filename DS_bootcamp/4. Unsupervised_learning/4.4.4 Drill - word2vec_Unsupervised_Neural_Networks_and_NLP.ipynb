{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayp\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import gensim\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # Visual inspection shows spaCy does not recognize the double dash '--'.\n",
    "    # Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    \n",
    "    # Get rid of headings in square brackets.\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    \n",
    "    # Get rid of chapter titles.\n",
    "    text = re.sub(r'Chapter \\d+','',text)\n",
    "    \n",
    "    # Get rid of extra whitespace.\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Import all the Austen in the Project Gutenberg corpus.\n",
    "austen = \"\"\n",
    "for novel in ['persuasion','emma','sense']:\n",
    "    work = gutenberg.raw('austen-' + novel + '.txt')\n",
    "    austen = austen + work\n",
    "\n",
    "# Clean the data.\n",
    "austen_clean = text_cleaner(austen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data. This can take some time.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "austen_doc = nlp(austen_clean[:999999])\n",
    "# Separating the corpus like this allows for combining them later and working around one million limit\n",
    "austen_doc2 = nlp(austen_clean[1000000:1999999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize the parsed doc into sentences, while filtering out punctuation\n",
    "# and stop words, and converting words to lower case lemmas.\n",
    "sentences = []\n",
    "for sentence in austen_doc.sents:\n",
    "    sentence = [\n",
    "        token.lemma_.lower()\n",
    "        for token in sentence\n",
    "        if not token.is_stop\n",
    "        and not token.is_punct\n",
    "    ]\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "for sentence in austen_doc2.sents:\n",
    "    sentence = [\n",
    "        token.lemma_.lower()\n",
    "        for token in sentence\n",
    "        if not token.is_stop\n",
    "        and not token.is_punct\n",
    "    ]\n",
    "    sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(\n",
    "    sentences,\n",
    "    workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=10,  # Minimum word count threshold.\n",
    "    window=6,      # Number of words around target word to consider.\n",
    "    sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    size=300,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('people', 0.651031494140625), ('settle', 0.5177574157714844), ('daughter', 0.4962022006511688), ('england', 0.4935835301876068), ('thousand', 0.4912043809890747), ('officer', 0.49020665884017944), ('introduction', 0.4836514890193939), ('mr', 0.48149317502975464), ('way', 0.47175687551498413), ('advantage', 0.4534520208835602)]\n",
      "0.8136536695330048\n",
      "0.21349597060459458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marriage\n"
     ]
    }
   ],
   "source": [
    "# List of words in model.\n",
    "vocab = model.wv.vocab.keys()\n",
    "\n",
    "print(model.wv.most_similar(positive=['lady', 'man'], negative=['woman']))\n",
    "\n",
    "# Similarity is calculated using the cosine, so again 1 is total\n",
    "# similarity and 0 is no similarity.\n",
    "print(model.wv.similarity('loud', 'aloud'))\n",
    "print(model.wv.similarity('mr', 'mrs'))\n",
    "\n",
    "# One of these things is not like the other...\n",
    "print(model.doesnt_match(\"breakfast marriage dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Drill 0\n",
    "\n",
    "Take a few minutes to modify the hyperparameters of this model and see how its answers change. Can you wrangle any improvements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(\n",
    "    sentences,\n",
    "    workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=10,  # Minimum word count threshold.\n",
    "    window=15,      # Number of words around target word to consider.\n",
    "    sg=1,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-4 ,  # Penalize frequent words.\n",
    "    size=100,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('eld', 0.9272082448005676), ('young', 0.9168800115585327), ('john', 0.9056689739227295), ('law', 0.9054642915725708), ('place', 0.9035821557044983), ('valuable', 0.8996951580047607), ('property', 0.8985298871994019), ('musical', 0.8956568837165833), ('donwell', 0.8946554064750671), ('consequence', 0.8913477063179016)]\n",
      "0.9836123881218409\n",
      "0.8498593800834731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marriage\n"
     ]
    }
   ],
   "source": [
    "# List of words in model.\n",
    "vocab = model.wv.vocab.keys()\n",
    "\n",
    "print(model.wv.most_similar(positive=['lady', 'man'], negative=['woman']))\n",
    "\n",
    "# Similarity is calculated using the cosine, so again 1 is total\n",
    "# similarity and 0 is no similarity.\n",
    "print(model.wv.similarity('loud', 'aloud'))\n",
    "print(model.wv.similarity('mr', 'mrs'))\n",
    "\n",
    "# One of these things is not like the other...\n",
    "print(model.doesnt_match(\"breakfast marriage dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Hyperparameter results\n",
    "The results significantly changed when altering the hyperparameters. We changed:\n",
    "\n",
    "- Word vector reduced to 100\n",
    "- 'sg' changed to 1\n",
    "- Window increased to 15\n",
    "\n",
    "# Example word2vec applications\n",
    "\n",
    "You can use the vectors from word2vec as features in other models, or try to gain insight from the vector compositions themselves.\n",
    "\n",
    "Here are some neat things people have done with word2vec:\n",
    "\n",
    " * [Visualizing word embeddings in Jane Austen's Pride and Prejudice](http://blogger.ghostweather.com/2014/11/visualizing-word-embeddings-in-pride.html). Skip to the bottom to see a _truly honest_ account of this data scientist's process.\n",
    "\n",
    " * [Tracking changes in Dutch Newspapers' associations with words like 'propaganda' and 'alien' from 1950 to 1990](https://www.slideshare.net/MelvinWevers/concepts-through-time-tracing-concepts-in-dutch-newspaper-discourse-using-sequential-word-vector-spaces).\n",
    "\n",
    " * [Helping customers find clothing items similar to a given item but differing on one or more characteristics](http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill 1: Word2Vec on 100B+ words\n",
    "\n",
    "However you access it, play around with a pretrained model. Is there anything interesting you're able to pull out about analogies, similar words, or words that don't match? Write up a quick note about your tinkering and discuss it with your mentor during your next session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format ('../../../backup/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.660988288657353\n",
      "0.38398625062987224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7885902371434435\n",
      "['good', 'really', 'great', 'love', 'happy', 'fun', 'unique', 'nice', 'perfect', 'enjoy', 'excited', 'proud', 'interesting', 'excellent', 'exciting', 'truly', 'beautiful', 'amazing', 'loved', 'tremendous', 'fantastic', 'liked', 'appreciate', 'sad', 'delighted', 'glad', 'incredible', 'extraordinary', 'remarkable', 'terrible', 'loves', 'brilliant', 'thrilled', 'exceptional', 'grateful', 'fortunate', 'joy', 'awesome', 'loving', 'memorable', 'terrific', 'superb', 'unbelievable', 'horrible', 'weird', 'finest', 'lovely', 'blessed', 'thankful', 'fascinating', 'rewarding', 'inspiring', 'phenomenal', 'fabulous', 'delicious', 'pleasant', 'enjoyable', 'magical', 'neat', 'magnificent', 'Wow', 'worthwhile', 'gorgeous', 'charming', 'refreshing', 'glorious', 'pleasing', 'delightful', 'breathtaking', 'appreciative', 'marvelous', 'unforgettable', 'wonderfully', 'bittersweet', 'gracious', 'gratifying', 'priceless', 'humbling', 'splendid', 'cherish', 'happiest', 'nicer', 'exhilarating', 'joyous', 'joyful', 'treasured', 'fond_memories', 'captivating', 'nicest', 'unreal', 'adore', 'Wonderful', 'enlightening', 'heartwarming', 'fantastically', 'awe_inspiring', 'enchanting', 'sweetest', 'incomparable', 'wondrous', 'thoroughly_enjoyed', 'greatly_appreciated', 'delectable', 'yummy', 'indescribable', 'fitting_tribute', 'proudest', 'scrumptious', 'saddens_me', 'enthuses', 'immensely_proud', 'eternally_grateful', 'warm_hearted', 'sheer_joy', 'loveliest', 'stunningly_beautiful', 'thoroughly_enjoyable', 'neatest', 'breathtakingly_beautiful', 'unbelieveable', 'sounds_corny', 'enormously_grateful', 'absolutely_adore', 'profoundly_grateful', 'cherish_forever', 'worthwhile_endeavor', 'justly_proud', 'forever_indebted', 'neatest_thing', 'fantabulous', 'phenominal', 'Applause._Thank', 'Christmasy', 'sounds_trite', 'richly_blessed', 'smashing_lad', 'wonderfull', 'purrfect', 'absolutely_adores', 'beatiful', 'doubly_blessed', 'agreat', 'Heartfelt_thanks', 'many_splendored', 'culturally_enriching', 'forgive_adultery', 'fellowshipping', 'fantasic', 'unforgetable', 'magnificant', 'includes_InterSystems_Jal', 'geat', 'Smoky_Oak_Taproom', 'CHRIS_ATTWOOD', 'eternally_thankful', 'sweetest_kindest', 'teriffic', 'wondeful', 'A.I', 'beautful', 'nicest_sweetest', 'Aahh', 'amzing', 'immeasurably_enriched', \"Today'sa\", 'sweetest_nicest', 'shelovesmodernwhomp', 'Iove', 'spiritually_enriching', 'inspring', 'phenomenol', 'la_dee_da', 'Ellen_Pinney', 'blessing_Chukwu', 'exclaim_Wow', 'Lis_Sladen', \"Everyone'sa_winner\", 'tells_BlogTalkRadio.com', 'bubbly_vivacious', 'beyond_nuisance_Warma', 'Angela_Garbiso_spokeswoman', 'DONVAN', 'ex_cellent', \"Every_day'sa\", 'ebullient_Ohno', 'tank_Bonfante', 'personable_likable', 'emotions_Vistan', 'safe_Arlene_Deche', 'kindest_warmest', 'Jacque_Callanen_elections', 'unstinting_generosity', 'Natasha_Richardson_mourned', 'dear_madam', 'bloomin_marvelous', 'goosebump_moments', 'wonderfully_readable', 'Winfrey_lavishing_praise', 'Sugarplum_Fairy_welcoming', 'loud_bang_Timms', 'greatly_exaggerated_Ferrer', 'remembered_Ankum', 'Huntsburg_visibly_moved', 'kindness_thoughtfulness', 'Besnik_Berisha_Pristina', 'fantastic_Boix', 'safe_Deche', 'Beverly_Boniello']\n",
      "0.7647868711574707\n",
      "3000000\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('mr', 'mrs'))\n",
    "print(model.similarity('loud', 'aloud'))\n",
    "print(model.wv.most_similar_to_given('wonderful', ['lacking', 'nice', 'good', 'perpetual']))\n",
    "print(model.wv.distance('what', 'govern'))\n",
    "print(model.words_closer_than('wonderful', 'ecstatic'))\n",
    "print(model.similarity('wonderful', 'great'))\n",
    "print(len(model.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More powerful model = More fun\n",
    "The pre-trained Google set obviously has more computational investment in its training. Is this valuable? I say yes.\n",
    "\n",
    "It seems to have captured the element of similarity in 'mr' and 'mrs', in 'loud' and 'aloud'. The pairs are not altogether dissimilar, that's correct, but they do not sit nearby one another as computed in the first models.\n",
    "\n",
    "A great deal of material is returned more similar than 'wonderful' and 'great', indicating this is a populated model. It looks to be trained on three million words (or word-grams). If this is somewhat outdated, I would love to see something more cutting-edge!"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "96px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
